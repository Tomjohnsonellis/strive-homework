{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76397619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some predictions...\n",
      "[0 0 1 2 0 2 2 2 2 1 0 1 2 1 1 2 1 1 1 0 0 0 1 0 0 2 2 1 0 2]\n",
      "These predictions are 76.67% accurate!\n",
      "-----\n",
      "CROSS VALIDATION\n",
      "Crossvalidation fold: 0  Accruacy: 0.8333333333333334\n",
      "Crossvalidation fold: 1  Accruacy: 0.9166666666666666\n",
      "Crossvalidation fold: 2  Accruacy: 0.75\n",
      "Crossvalidation fold: 3  Accruacy: 0.75\n",
      "Crossvalidation fold: 4  Accruacy: 0.7916666666666666\n",
      "Mean train cross validation score 0.8083333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def load_data():\n",
    "    #load the dataset\n",
    "    #return the dataset\n",
    "    return datasets.load_iris()\n",
    "\n",
    "\n",
    "def dataset_to_pandas():\n",
    "    #put the dataset into a pandas DF using the feature names as columns√ß\n",
    "    #rename the column name so the dont include the '(cm)'\n",
    "    #add 2 columns one with the target and another with the class\n",
    "    \n",
    "    # First attempt was columns=new_columns, but then I remembered list comprehensions exist\n",
    "    #     new_columns = [feature.strip(\" (cm)\") for feature in dataset.feature_names]\n",
    "\n",
    "    # Initially the dataframe is just the sepal and petal data\n",
    "    df = pd.DataFrame(dataset.data.tolist(), columns=[feature.strip(\" (cm)\") for feature in dataset.feature_names])\n",
    "    # For some reason numpy ndarrays can't use .replace() so I put the data into a series.\n",
    "    target_series = pd.Series(dataset.target)\n",
    "    class_series = target_series.replace({0: \"setosa\", 1:\"versicolor\",2:\"virginica\"})\n",
    "    # Finally add these new series to the dataframe\n",
    "    df[\"target\"] = target_series\n",
    "    df[\"class\"] = class_series\n",
    "    return df\n",
    "\n",
    "dataset = load_data()\n",
    "df = dataset_to_pandas()\n",
    "\n",
    "def target_to_numpy():\n",
    "    target_array = np.array(df.target)\n",
    "    return target_array\n",
    "def data_to_numpy():\n",
    "    data_array = np.array([ df[\"sepal length\"],df[\"sepal width\"] ] ).transpose()\n",
    "    return data_array\n",
    "# Create the data arrays\n",
    "Y = target_to_numpy()\n",
    "X = data_to_numpy()\n",
    "\n",
    "\"\"\"\n",
    "############################################## Pipeline below\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline\n",
    "# X is the data/parameters/info etc, the things we make predictions FROM\n",
    "# Y is the result we are attempting to predict. type of flower, price, weather etc.\n",
    "\n",
    "\n",
    "# Split up the data, leaving some to test on\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.8)\n",
    "\n",
    "# Data doesn't always need to be normalised!\n",
    "# # Normalise the data to make it all the same range\n",
    "# X_train = preprocessing.normalize(X_train)\n",
    "# X_test = preprocessing.normalize(X_test)\n",
    "\n",
    "#create and fit the scaler object on the training data\n",
    "scaler = StandardScaler()\n",
    "# Fit the model to the training data\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Make predictions using logistic regression, on the training data\n",
    "clf = LogisticRegression(solver=\"lbfgs\",multi_class=\"multinomial\").fit(X_train, Y_train)\n",
    "# Score using the test data\n",
    "\n",
    "predicitions = clf.predict(X_test)\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"Here are some predictions...\")\n",
    "print(predicitions)\n",
    "print(\"These predictions are {}% accurate!\".format(round(score*100,2)))\n",
    "\n",
    "\n",
    "cv = cross_validate(clf, X_train, Y_train)\n",
    "\n",
    "def print_scores(cv):\n",
    "    #print out cross validation scores\n",
    "    print(\"-----\")\n",
    "    print(\"CROSS VALIDATION\")\n",
    "    [print('Crossvalidation fold: {}  Accruacy: {}'.format(n, score)) for n, score in enumerate(cv['test_score'])]\n",
    "    #print out the mean of the cross validation\n",
    "    print('Mean train cross validation score {}'.format(cv['test_score'].mean()))\n",
    "\n",
    "print_scores(cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
