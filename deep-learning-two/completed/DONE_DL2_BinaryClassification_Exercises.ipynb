{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "hazardous-weather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "clean-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "net = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass\n",
    "\n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "demographic-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "curious-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super().__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "recreational-macro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:32.252906Z",
     "start_time": "2021-05-26T07:56:32.247913Z"
    }
   },
   "outputs": [],
   "source": [
    "assert NeuralNetwork(input_dim=10, num_hidden=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bigger-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "hawaiian-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:48.612004Z",
     "start_time": "2021-05-26T08:04:48.606773Z"
    }
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(16, 16, 16)\n",
    "assert net.fc1\n",
    "assert net.fc2\n",
    "assert net.fc3\n",
    "assert net.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "smart-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "# you defined before. Follow the same reasoning we used in class\n",
    "\n",
    "# As above\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Added\n",
    "    def forward(self, data_given_to_network):\n",
    "        starting_score = self.fc1(data_given_to_network)\n",
    "        initial_sigmoid = self.sigmoid(starting_score)\n",
    "        score2 = self.fc2(initial_sigmoid)\n",
    "        sigmoid2 = self.sigmoid(score2)\n",
    "        score3 = self.fc3(sigmoid2)\n",
    "        network_output = self.sigmoid(score3)\n",
    "        return network_output\n",
    "\n",
    "model = NeuralNetwork(5,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "latest-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "lesser-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "Epoch 1: Loss = 0.7452790141105652\n",
      "Epoch 2: Loss = 0.7506649494171143\n",
      "Epoch 3: Loss = 0.7199462652206421\n",
      "Epoch 4: Loss = 0.6942074298858643\n",
      "Epoch 5: Loss = 0.6941081285476685\n",
      "Epoch 6: Loss = 0.7004321813583374\n",
      "Epoch 7: Loss = 0.7039613127708435\n",
      "Epoch 8: Loss = 0.7035173773765564\n",
      "Epoch 9: Loss = 0.7005257606506348\n",
      "Epoch 10: Loss = 0.6966824531555176\n"
     ]
    }
   ],
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", header=None)\n",
    "#print(data)\n",
    "#print(data[0])\n",
    "X = data.drop(2, axis=1).values\n",
    "y = data[2].values\n",
    "#print(X)\n",
    "#print(y)\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "y = y.view(100, 1)\n",
    "print(X.size())\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "def train_model(X, y, model, loss_func, lr=0.1, num_epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_predictions = model(X)\n",
    "        loss_value = loss_func(y_predictions, y)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_value}\")\n",
    "\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "#print(X.size()[0])\n",
    "\n",
    "untrained_model = NeuralNetwork(input_dim=2, num_hidden1=10, num_hidden2=10)\n",
    "trained_model = train_model(X, y, untrained_model,loss, lr=0.2, num_epochs=10)\n",
    "\n",
    "# \"\"\"\n",
    "# Note to self: Check your variable names VERY carefully, save yourself 30 mins of random debugging\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.6958317756652832\n",
      "Epoch 2: Loss = 0.9101396799087524\n",
      "Epoch 3: Loss = 0.7070919871330261\n",
      "Epoch 4: Loss = 0.6786473989486694\n",
      "Epoch 5: Loss = 0.7056552171707153\n",
      "Epoch 6: Loss = 0.688127875328064\n",
      "Epoch 7: Loss = 0.6476816534996033\n",
      "Epoch 8: Loss = 0.6140370965003967\n",
      "Epoch 9: Loss = 0.5954224467277527\n",
      "Epoch 10: Loss = 0.5870546698570251\n",
      "Epoch 11: Loss = 0.5779760479927063\n",
      "Epoch 12: Loss = 0.5564829111099243\n",
      "Epoch 13: Loss = 0.5184482336044312\n",
      "Epoch 14: Loss = 0.47088754177093506\n",
      "Epoch 15: Loss = 0.42852774262428284\n",
      "Epoch 16: Loss = 0.40420886874198914\n",
      "Epoch 17: Loss = 0.3875580132007599\n",
      "Epoch 18: Loss = 0.3538874685764313\n",
      "Epoch 19: Loss = 0.30820634961128235\n",
      "Epoch 20: Loss = 0.2741309404373169\n",
      "Epoch 21: Loss = 0.2573927938938141\n",
      "Epoch 22: Loss = 0.24174395203590393\n",
      "Epoch 23: Loss = 0.21794898808002472\n",
      "Epoch 24: Loss = 0.19634708762168884\n",
      "Epoch 25: Loss = 0.18898658454418182\n",
      "Epoch 26: Loss = 0.18530477583408356\n",
      "Epoch 27: Loss = 0.1719132661819458\n",
      "Epoch 28: Loss = 0.15964001417160034\n",
      "Epoch 29: Loss = 0.15714986622333527\n",
      "Epoch 30: Loss = 0.1564057618379593\n",
      "Epoch 31: Loss = 0.1504163295030594\n",
      "Epoch 32: Loss = 0.14411525428295135\n",
      "Epoch 33: Loss = 0.1437613070011139\n",
      "Epoch 34: Loss = 0.14516384899616241\n",
      "Epoch 35: Loss = 0.14214402437210083\n",
      "Epoch 36: Loss = 0.13877233862876892\n",
      "Epoch 37: Loss = 0.1394689977169037\n",
      "Epoch 38: Loss = 0.14083173871040344\n",
      "Epoch 39: Loss = 0.13941575586795807\n",
      "Epoch 40: Loss = 0.1376366913318634\n",
      "Epoch 41: Loss = 0.13850118219852448\n",
      "Epoch 42: Loss = 0.13971632719039917\n",
      "Epoch 43: Loss = 0.13867607712745667\n",
      "Epoch 44: Loss = 0.13780543208122253\n",
      "Epoch 45: Loss = 0.13877460360527039\n",
      "Epoch 46: Loss = 0.13939806818962097\n",
      "Epoch 47: Loss = 0.1385958194732666\n",
      "Epoch 48: Loss = 0.13824906945228577\n",
      "Epoch 49: Loss = 0.1390722393989563\n",
      "Epoch 50: Loss = 0.1392168253660202\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", header=None)\n",
    "#print(data)\n",
    "#print(data[0])\n",
    "X = data.drop(2, axis=1).values\n",
    "y = data[2].values\n",
    "#print(X)\n",
    "#print(y)\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "y = y.view(100, 1)\n",
    "\n",
    "def train_our_simple_network(data, labels, our_model, some_loss_function, some_learning_rate, number_of_epochs):\n",
    "    # Typical practice is to define the optimiser inside the training function, as that is where it will be used\n",
    "    optimiser = torch.optim.Adam(our_model.parameters(), lr=some_learning_rate)\n",
    "    for epoch in range(number_of_epochs):\n",
    "        optimiser.zero_grad() # This just ensures that we recalculate the gradients from scratch each epoch\n",
    "        label_predictions = our_model(data) # Make some predictions\n",
    "        loss_value = some_loss_function(label_predictions, labels)  # Calculate how good they were\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_value}\")\n",
    "\n",
    "        loss_value.backward() # Adjust the network accordingly\n",
    "        optimiser.step() # Updates our parameters\n",
    "    return our_model\n",
    "\n",
    "\n",
    "class A_Simple_Neural_Network_With_A_Forward_Pass(nn.Module):\n",
    "    def __init__(self, input_dimensions, hidden_layer_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.first_linear_calculation = nn.Linear(input_dimensions, hidden_layer_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.second_linear_calculation = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, data_given_to_the_network):\n",
    "        results_of_first_linear_calculation = self.first_linear_calculation(data_given_to_the_network)\n",
    "        activation_results = self.sigmoid(results_of_first_linear_calculation)\n",
    "        results_of_second_linear_calculation = self.second_linear_calculation(activation_results)\n",
    "        network_output = self.sigmoid(results_of_second_linear_calculation)\n",
    "        return network_output\n",
    "\n",
    "\n",
    "simple_net = A_Simple_Neural_Network_With_A_Forward_Pass(2, 10, 1)\n",
    "trained_network = train_our_simple_network(X, y, simple_net, nn.BCELoss(), 0.25, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}