{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "_How can we re-use models that have already been trained?_<br>\n",
    "Depending on the task, we may want to salvage other models for parts, perhaps we're expanding functionality or have a similar problem to one we've solved before. Instead of starting from scratch every time, depending on the task, we can use pre-trained models as a starting point and then tweak them to our specific needs.\n",
    "<p>\n",
    "Convolutional NNs are often used for feature extraction, with the layers starting with low level features like edges and shapes, and progressing up to more advanced features. We can simply tune a previously trained model's weights with a new dataset to achieve decent results much faster than retraining from scratch.\n",
    "<p>\n",
    "Typically we will need to train a new classifier, as in most cases we will be using a model to detect new classes and possibly a different amount of total classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: majority of pre-trained models from torchvision require images to be 224x224px in size, but other models may have different requirements.* We will also need to perform the same normalisation used when the model was being trained.\n",
    "<br>***Make sure to understand how each model you use actually works and what inputs it requires.***\n",
    "<p>For this model specifically, each color channel was normalised seperately.\n",
    "<br>Means: [0.485, 0.456, 0.406]\n",
    "<br>Standard Deviations: [0.229, 0.224, 0.225]\n",
    "<p> Let's start by loading our data, in this case we will be using Cats and Dogs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "data_dir = \"data/Cat_Dog_data\"\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Resize the data and augment it, then normalise\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "# Resize and normalise\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "import os\n",
    "train_data = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=test_transforms)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=1664, out_features=1000, bias=True)"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch offers a variety of pretrained models https://pytorch.org/docs/0.3.0/torchvision/models.html\n",
    "# We will be using one called \"DenseNet\"\n",
    "model = models.densenet169(pretrained=True)\n",
    "# You can take a look at a massive list of information with...\n",
    "model\n",
    "# But the main thing we care about is the number of outputs the model gives,\n",
    "# As that is what we need to pass to a classifier\n",
    "model.classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The DenseNet model was trained using the [ImageNet](https://image-net.org) dataset, which is over 14 million images, although the classifications are not the same, we can use the feature detection part of the model and just attach a new classifier to it.\n",
    "<br>_Note: For image recognition, pre-trained models like this are VERY good at detection features, we should make use of them!_\n",
    "<p>As this model is already sufficient we have no need to adjust it, so we will lock/freeze it by disabling gradient calculation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will make our own classifier for the Cats and Dogs data we are using\n",
    "<br>_Remember: The classifier needs to accept the output of the model!_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# Input size required: 1664\n",
    "cat_dog_classifier = nn.Sequential(OrderedDict([\n",
    "    (\"fc1\", nn.Linear(in_features=1664, out_features=64)),\n",
    "    (\"relu\",nn.ReLU()),\n",
    "    (\"fc2\",nn.Linear(in_features=64, out_features=10)),\n",
    "    (\"output\", nn.LogSoftmax(dim=1)),\n",
    "]))\n",
    "\n",
    "# Overwrite the old classifier\n",
    "model.classifier = cat_dog_classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the new classifier\n",
    "This model is incredibly complex compared to what we are used to, were now at the point where performance and memory usage are pretty important. So we will be using CUDA if available to speed up processing.\n",
    "<br>_The difference can be factors of 100, use whenever possible_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "#torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "# Recap\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use cuda\n",
    "model = models.densenet169(pretrained=True) # Load a pretrained model\n",
    "for param in model.parameters(): # Freeze the model\n",
    "    param.requires_grad = False\n",
    "model.classifier = cat_dog_classifier # Replace the old classifier\n",
    "criterion = nn.NLLLoss() # Choose a loss function\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003) # Optimise only the classifier\n",
    "model.to(device); # Move it all to the GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# epochs = 3\n",
    "# steps = 0\n",
    "# running_loss = 0\n",
    "# print_every = 50\n",
    "# max_accuracy = 0\n",
    "# accuracy_results = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch {epoch+1}\")\n",
    "#     print(steps)\n",
    "#     for inputs, labels in trainloader:\n",
    "#         steps += 1\n",
    "#         print(f\"Step: {steps}\")\n",
    "#         # Move input and label tensors to the default device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#\n",
    "#         # write the training loop. call the loss \"loss\" so that the line below will work\n",
    "#         #print(\"> Making predictions\")\n",
    "#         predictions = model(inputs)\n",
    "#         #print(\"> Calculating loss\")\n",
    "#         loss = criterion(predictions, labels)\n",
    "#         #print(\"> Backpropogating\")# Compute the loss\n",
    "#         loss.backward()\n",
    "#         #print(\"> Stepping\")# Backpropogate\n",
    "#         optimizer.step()\n",
    "#         #print(\"> Resetting gradients\")# Update the parameters (weights and biases)\n",
    "#         optimizer.zero_grad() # Reset gradients\n",
    "#\n",
    "#         running_loss += loss.item()\n",
    "#         #print(f\"> Running loss: {running_loss}\")\n",
    "#\n",
    "#         if steps % print_every == 0:\n",
    "#             test_loss = 0\n",
    "#             accuracy = 0\n",
    "#             print(\"~ Changing to evaluation mode\")\n",
    "#             model.eval()\n",
    "#             # ...\n",
    "#             with torch.no_grad():\n",
    "#                 for inputs, labels in testloader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     logps = model.forward(inputs)\n",
    "#                     batch_loss = criterion(logps, labels)\n",
    "#                     test_loss += batch_loss.item()\n",
    "#\n",
    "#                     # Calculate accuracy\n",
    "#                     ps = torch.exp(logps)\n",
    "#                     top_p, top_class = ps.topk(1, dim=1)\n",
    "#                     equals = top_class == labels.view(*top_class.shape)\n",
    "#                     accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "#                     # Save the best model\n",
    "#                     if accuracy >= max_accuracy:\n",
    "#                         max_accuracy = accuracy\n",
    "#                         torch.save(model.state_dict(), 'catordog.epic')\n",
    "#\n",
    "#\n",
    "#             print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "#                   f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "#                   f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "#                   f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "#             accuracy_results.append([steps, accuracy/len(testloader)])\n",
    "#             running_loss = 0\n",
    "#             print(\"~ Reverting to training mode\")\n",
    "#             model.train()\n",
    "        # REMEMBER TO REACTIVATE THE TRAIN MODE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "for step, result in accuracy_results:\n",
    "    print(f\"@ Step {step}: {result} accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretty graphs\n",
    "The most important thing about data science is obviously how cool it looks.\n",
    "[wandb.ai](wandb.ai) is a visualisation tool for models that looks quite flashy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Ignored wandb.init() arg project when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">stellar-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tjexyz/uncategorized\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/tjexyz/uncategorized/sweeps/n0epyshv\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/sweeps/n0epyshv</a><br/>\nRun page: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/4awkd3xu\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/4awkd3xu</a><br/>\n                Run data is saved locally in <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170403-4awkd3xu</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'epochs' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hy81zycv\n",
      "Sweep URL: https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 3iqz7w0a with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 4\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.022134662423268673\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">desert-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tjexyz/uncategorized\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv</a><br/>\nRun page: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/3iqz7w0a\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/3iqz7w0a</a><br/>\n                Run data is saved locally in <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170411-3iqz7w0a</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 16040<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47f6d48ebba44da294e0d4425f5f834e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170411-3iqz7w0a\\logs\\debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170411-3iqz7w0a\\logs\\debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">desert-sweep-1</strong>: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/3iqz7w0a\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/3iqz7w0a</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Run 3iqz7w0a errored: NameError(\"name 'make_model' is not defined\")\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 1k7u146j with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.07058362064167256\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">comfy-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tjexyz/uncategorized\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv</a><br/>\nRun page: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/1k7u146j\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/1k7u146j</a><br/>\n                Run data is saved locally in <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170423-1k7u146j</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 12784<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a7afeb0cd8940d58e647fa4a55aeff9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170423-1k7u146j\\logs\\debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170423-1k7u146j\\logs\\debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">comfy-sweep-2</strong>: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/1k7u146j\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/1k7u146j</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Run 1k7u146j errored: NameError(\"name 'make_model' is not defined\")\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: lrxqftns with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 1\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.023572562475724364\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">royal-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tjexyz/uncategorized\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/sweeps/hy81zycv</a><br/>\nRun page: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/lrxqftns\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/lrxqftns</a><br/>\n                Run data is saved locally in <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170434-lrxqftns</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 15192<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47f36dc631844edebf3c9e3384006ce6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170434-lrxqftns\\logs\\debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>C:\\Users\\tom\\Documents\\GitHub\\strive-work\\deep-learning\\wandb\\run-20210603_170434-lrxqftns\\logs\\debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">royal-sweep-3</strong>: <a href=\"https://wandb.ai/tjexyz/uncategorized/runs/lrxqftns\" target=\"_blank\">https://wandb.ai/tjexyz/uncategorized/runs/lrxqftns</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Run lrxqftns errored: NameError(\"name 'make_model' is not defined\")\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"gpt-3\")\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "config.epochs = 10\n",
    "wandb.watch(model)\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\":\"basic sweep\",\n",
    "    \"method\":\"random\",\n",
    "    \"parameters\":{\n",
    "        \"epochs\":{\n",
    "            \"values\":[1,2,3,3,3,4,5,5]\n",
    "        },\n",
    "        \"learning_rate\":{\n",
    "            \"min\":0.0001,\n",
    "            \"max\":0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "def wandb_train():\n",
    "    with wandb.init() as run:\n",
    "        cofig = wandb.config\n",
    "        model = make_model(config)\n",
    "        running_loss = 0\n",
    "\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            steps = 0\n",
    "            print_every = 10\n",
    "            accuracy = 0\n",
    "            test_loss = 0\n",
    "\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(inputs)\n",
    "                loss = criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                test_loss += batch_loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                ps = torch.exp(logps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                #wandb.log({\"accuracy\":accuracy})\n",
    "            #loss = model.fit() # your model training code here\n",
    "            wandb.log({\"accuracy\":accuracy, \"epoch\":epoch})\n",
    "\n",
    "count = 100\n",
    "wandb.agent(sweep_id, function=wandb_train, count=count)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ee' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-135-0c2876394226>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mee\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'ee' is not defined"
     ]
    }
   ],
   "source": [
    "ee"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}