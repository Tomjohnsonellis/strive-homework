{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning: Reference Sheet\n",
    "***\n",
    "![Subset Diagram](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/subsets.png?raw=true)\n",
    "<br>***Artificial Intelligence:*** Programs with the ability to learn and reason like humans\n",
    "<br>***Machine Learning:*** Algorithms with the ability to \"learn\" without being explicitly programmed. Mathematical processes count for this, linear regression, backpropogation etc.\n",
    "<br>***Deep Learning:*** A subset of machine learning, essentially when neural networks has 3+ layers, we would call it a deep learning model.\n",
    "***\n",
    "## Common Neural Network Terms\n",
    "- Activation Function\n",
    "<br>*What?* Applied to the output of a neuron to determine how \"activated\" it is, for example [ReLU](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units) or [Sigmoid](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function).\n",
    "<br>*Why?* These numerically show what neurons are being used for the current inputs, fundamental in training networks!\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)*\n",
    "```python\n",
    "# Typically used in a sequential model on a layer's outputs and then passed to the next layer\n",
    "torch.nn.ReLU(inplace=False)\n",
    "\n",
    "```\n",
    "\n",
    "- [Softmax](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)\n",
    "<br>Takes a raw output vector and converts it to numbers that add up to 1, allowing you to use it as a probability.\n",
    "<br>![Softmax Equation](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/softmax.png?raw=true)\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)*\n",
    "```python\n",
    "torch.nn.Softmax(outputs_of_a_neural_network, dim=dimension_to_softmax)\n",
    "# Or\n",
    "softmax = torch.nn.Softmax()\n",
    "softmaxed_outputs = softmax(outputs_of_a_neural_network)\n",
    "```\n",
    "- Dropout\n",
    "<br>*What?* Randomly deactivates a percentage of neurons when training a NN.\n",
    "<br>*Why?* This helps prevent models from overfitting to the training data. Without dropout, optimisations (like gradient descent) are applied to the entire network at each step, this can lead to neurons fixing the mistakes of other neurons - resulting in complicated co-adaptations which don't generalise well.\n",
    "- Batch Normalisation\n",
    "- Loss Function\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [-2.5000],\n",
      "        [ 4.0000]])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor([ [1], [-1], [0], [-2.5], [4]])\n",
    "f = torch.nn.ReLU()\n",
    "print(x)\n",
    "print(f(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}