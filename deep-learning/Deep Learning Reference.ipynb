{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning: Reference Sheet\n",
    "***\n",
    "![Subset Diagram](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/subsets.png?raw=true)\n",
    "<br>***Artificial Intelligence:*** Programs with the ability to learn and reason like humans\n",
    "<br>***Machine Learning:*** Algorithms with the ability to \"learn\" without being explicitly programmed. Mathematical processes count for this, linear regression, backpropogation etc.\n",
    "<br>***Deep Learning:*** A subset of machine learning, essentially when neural networks has 3+ layers, we would call it a deep learning model.\n",
    "***\n",
    "## Common Neural Network Terms\n",
    "- Activation Function\n",
    "<br>*What?* Applied to the output of a neuron to determine how \"activated\" it is, for example [ReLU](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units) or [Sigmoid](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function).\n",
    "<br>![Activation Functions](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/activations.png?raw=true)\n",
    "<br>*Why?* These numerically show what neurons are being used for the current inputs, fundamental in training networks!\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)*\n",
    "```python\n",
    "# Typically used in a sequential model on a layer's outputs and then passed to the next layer\n",
    "torch.nn.ReLU(inplace=False)\n",
    "```\n",
    "- [Softmax](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)\n",
    "<br>*What?* Takes a raw output vector and converts it to numbers that add up to 1.\n",
    "<br>![Softmax Equation](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/softmax.png?raw=true)\n",
    "<br>*Why?* The raw outputs from a NN will probably be arbitray numbers, so this function converts them to a usable result for probabilities.\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)*\n",
    "```python\n",
    "torch.nn.Softmax(outputs_of_a_neural_network, dim=dimension_to_softmax)\n",
    "# Or\n",
    "softmax = torch.nn.Softmax()\n",
    "softmaxed_outputs = softmax(outputs_of_a_neural_network)\n",
    "```\n",
    "- Dropout\n",
    "<br>*What?* Randomly deactivates a percentage of neurons when training a NN. Different neurons each step\n",
    "<br>![Dropout](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/dropout.gif?raw=true)\n",
    "<br>*Why?* This helps prevent models from overfitting to the training data. Without dropout, optimisations (like gradient descent) are applied to the entire network at each step, this can lead to neurons fixing the mistakes of other neurons - resulting in complicated co-adaptations which don't generalise well.\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)*\n",
    "<br>[>>Example Model<<](https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE)\n",
    "```python\n",
    "self.dropout = nn.Dropout(percentage_of_neurons_to_drop)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- Batch Normalisation\n",
    "<br>*What?* In addition to normalising the inputs to the network, we normalise the inputs to *each layer* with the mean/variance of the current batch's values.\n",
    "<br>*Why?* Faster and more stable training - [Paper](https://arxiv.org/pdf/1502.03167.pdf) - In short, it helps combat Internal Covariate Shift, which is where the distribution of inputs changes from layer-to-layer, which is bad.\n",
    "<br>*How?* [2D or 3D inputs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) / [4D inputs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "<br>[>>Article<<](https://www.machinecurve.com/index.php/2021/03/29/batch-normalization-with-pytorch/)\n",
    "```python\n",
    "# Typically used in a sequential model on the outputs of a layer, then passed to the activation function\n",
    "nn.BatchNorm1d(number_of_outputs_from_layer)\n",
    "```\n",
    "\n",
    "- Loss Function\n",
    "<br>*What?* A function to calculate the error of a model (*How* incorrect it is)\n",
    "<br>*Why?* This is how we determine that a model is learning, you know, *the entire point of machine learning*.\n",
    "<br>*How?*\n",
    "```python\n",
    "# Mean-Squared Error, used in regression https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "score_criteria = nn.MSELoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "# Negative Log-Likelihood Loss, used in classification (remember to use LogSoftmax!) https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "score_criteria = nn.NLLLoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "# Cross Entropy Loss, classification but doesn't need a softmax on the outputs https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "score_criteria = nn.CrossEntropyLoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "```\n",
    "- Optimizer\n",
    "<br>*What?* An algorithm that updates our model to reduce error\n",
    "<br>*Why?* This is how we improve our models!\n",
    "<br>*[How?](https://pytorch.org/docs/stable/optim.html)*\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=a_learning_rate)\n",
    "optimizer = torch.optim.Adam(\n",
    "# Used as part of the training loop\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "***\n",
    "#### Other Definitions\n",
    "***Logits***: The output of a hidden layer before applying an activation function or softmax. Raw Outputs\n",
    "I<br>***Backpropogation***: The algorithm performed by neural networks to compute the gradient of the loss function. *It does not refer to how the gradient is used.*\n",
    "<br>***Output Probabilities***: The probability of each class in a NN's prediction e.g. 99% dog, 1% cat\n",
    "***\n",
    "# How does a neural network learn?\n",
    "*In short...*\n",
    "0. We initialise a network with random weights and biases, or the values from a pretrained model\n",
    "1. Data is passed forward through the network\n",
    "2. The network will output some result (probably completely incorrect)\n",
    "3. We compute the error (*how* wrong the result is)\n",
    "4. Using this, we backpropogate to calculate the gradient of the loss function with respect to the weights\n",
    "5. We use this gradient to determine how much to update the weights and biases by (along with the learning rate)\n",
    "6. We apply the changes to the weights\n",
    "<p>That's it! Steps 1 to 6 are then repeated until we're happy with the model\n",
    "***\n",
    "## Categories of Deep Learning architectures\n",
    "***Multilayer Perceptron***: The basic neural network composed entirely of fully connected layers.\n",
    "<br>![MLP](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/multilayerperceptron.jpg?raw=true)\n",
    "***Convolutional Neural Network***: Used on grid-like data, images, movies, audio. These networks are specialised for extracting *[features](https://en.wikipedia.org/wiki/Feature_%28machine_learning%29)*\n",
    "![CNN layer](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/cnn.gif?raw=true)\n",
    "<br> The typical architecture is [Inputs]-> Convolutional layer --> Max pooling --> Convolutional layer --> Max pooling --> [Classifier]\n",
    "<br> CNNs are trained to detect features, those features are then passed to a classifier that determines \"what they mean\".\n",
    "<br>***Recurrent Neural Networks***: Used on time-series data, usually trying to solve a \"What's the next value in this series?\" problems\n",
    "<br>[>>>Wikipedia<<<](https://en.wikipedia.org/wiki/Recurrent_neural_network#LSTM) - Basically, they allow time to be a factor that affects them, so are very effective for speech recognition, machine translation, music composition, handwriting recognition...\n",
    "<br> By increasing or decreasing the impact of information based on how recent it was, RNNs are far more effective than other networks for time-series tasks.\n",
    "<br>![RNN](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/rnn.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Todo\n",
    "Generic training loop\n",
    "Model examples using pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}