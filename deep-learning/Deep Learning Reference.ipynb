{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning: Reference Sheet\n",
    "***\n",
    "![Subset Diagram](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/subsets.png?raw=true)\n",
    "<br>***Artificial Intelligence:*** Programs with the ability to learn and reason like humans\n",
    "<br>***Machine Learning:*** Algorithms with the ability to \"learn\" without being explicitly programmed. Mathematical processes count for this, linear regression, backpropogation etc.\n",
    "<br>***Deep Learning:*** A subset of machine learning, essentially when neural networks has 3+ layers, we would call it a deep learning model.\n",
    "***\n",
    "## Common Neural Network Terms\n",
    "- Activation Function\n",
    "<br>*What?* Applied to the output of a neuron to determine how \"activated\" it is, for example [ReLU](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units) or [Sigmoid](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function).\n",
    "<br>![Activation Functions](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/activations.png?raw=true)\n",
    "<br>*Why?* These numerically show what neurons are being used for the current inputs, fundamental in training networks!\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)*\n",
    "```python\n",
    "# Typically used in a sequential model on a layer's outputs and then passed to the next layer\n",
    "torch.nn.ReLU(inplace=False)\n",
    "```\n",
    "- [Softmax](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)\n",
    "<br>*What?* Takes a raw output vector and converts it to numbers that add up to 1.\n",
    "<br>![Softmax Equation](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/softmax.png?raw=true)\n",
    "<br>*Why?* The raw outputs from a NN will probably be arbitray numbers, so this function converts them to a usable result for probabilities.\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)*\n",
    "```python\n",
    "torch.nn.Softmax(outputs_of_a_neural_network, dim=dimension_to_softmax)\n",
    "# Or\n",
    "softmax = torch.nn.Softmax()\n",
    "softmaxed_outputs = softmax(outputs_of_a_neural_network)\n",
    "```\n",
    "- Dropout\n",
    "<br>*What?* Randomly deactivates a percentage of neurons when training a NN. Different neurons each step\n",
    "<br>![Dropout](https://github.com/Tomjohnsonellis/strive-work/blob/main/deep-learning/img/dropout.gif?raw=true)\n",
    "<br>*Why?* This helps prevent models from overfitting to the training data. Without dropout, optimisations (like gradient descent) are applied to the entire network at each step, this can lead to neurons fixing the mistakes of other neurons - resulting in complicated co-adaptations which don't generalise well.\n",
    "<br>*[How?](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)*\n",
    "<br>[>>Example Model<<](https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE)\n",
    "```python\n",
    "self.dropout = nn.Dropout(percentage_of_neurons_to_drop)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- Batch Normalisation\n",
    "<br>*What?* In addition to normalising the inputs to the network, we normalise the inputs to *each layer* with the mean/variance of the current batch's values.\n",
    "<br>*Why?* Faster and more stable training - [Paper](https://arxiv.org/pdf/1502.03167.pdf) - In short, it helps combat Internal Covariate Shift, which is where the distribution of inputs changes from layer-to-layer, which is bad.\n",
    "<br>*How?* [2D or 3D inputs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) / [4D inputs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "<br>[>>Article<<](https://www.machinecurve.com/index.php/2021/03/29/batch-normalization-with-pytorch/)\n",
    "```python\n",
    "# Typically used in a sequential model on the outputs of a layer, then passed to the activation function\n",
    "nn.BatchNorm1d(number_of_outputs_from_layer)\n",
    "```\n",
    "\n",
    "- Loss Function\n",
    "<br>*What?* A function to calculate the error of a model (*How* incorrect it is)\n",
    "<br>*Why?* This is how we determine that a model is learning, you know, *the entire point of machine learning*.\n",
    "<br>*How?*\n",
    "```python\n",
    "# Mean-Squared Error, used in regression https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "score_criteria = nn.MSELoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "# Negative Log-Likelihood Loss, used in classification (remember to use LogSoftmax!) https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "score_criteria = nn.NLLLoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "# Cross Entropy Loss, classification but doesn't need a softmax on the outputs https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "score_criteria = nn.CrossEntropyLoss()\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "```\n",
    "- Optimizer\n",
    "<br>*What?* An algorithm that updates our model to reduce error\n",
    "<br>*Why?* This is how we improve our models!\n",
    "<br>*[How?](https://pytorch.org/docs/stable/optim.html)*\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=a_learning_rate)\n",
    "optimizer = torch.optim.Adam(\n",
    "# Used as part of the training loop\n",
    "loss = score_criteria(model_predictions, actual_values)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [-2.5000],\n",
      "        [ 4.0000]])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [4.]])\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [-2.5000],\n",
      "        [ 4.0000]])\n",
      "tensor([[ 0.0000],\n",
      "        [-1.3333],\n",
      "        [ 0.0000],\n",
      "        [-3.3333],\n",
      "        [ 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor([ [1], [-1], [0], [-2.5], [4]])\n",
    "f = torch.nn.ReLU()\n",
    "print(x)\n",
    "print(f(x))\n",
    "\n",
    "print(x)\n",
    "g = torch.nn.Dropout(p=0.25)\n",
    "print(g(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}